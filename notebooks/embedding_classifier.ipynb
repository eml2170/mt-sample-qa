{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "import sys\n",
    "sys.path.append('../code')\n",
    "from utils import preprocess_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/mtsamples.csv\")\n",
    "df['transcription_clean'] = df['transcription'].apply(preprocess_text)\n",
    "\n",
    "vocab = {}\n",
    "for _, row in df.iterrows():\n",
    "    x = row[\"transcription_clean\"]\n",
    "    for word in x.split():\n",
    "        vocab[word] = vocab.get(word, 0) + 1\n",
    "len(vocab)\n",
    "\n",
    "k = 10  \n",
    "vocab_filtered = [word for word in vocab if vocab[word] > k]  # same order of magnitude as n\n",
    "len(vocab_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_specialties = df.medical_specialty.value_counts()\n",
    "top_specialties = top_specialties[top_specialties > 100]\n",
    "classes = list(top_specialties.keys())\n",
    "prev_count = len(df)\n",
    "df = df.loc[df.medical_specialty.isin(classes)]\n",
    "print(f\"Went from {prev_count} samples to {len(df)} samples\")\n",
    "\n",
    "class_dict = {c:i for i, c in enumerate(classes)}\n",
    "\n",
    "def one_hot_encode(specialty):\n",
    "    y = np.zeros(len(classes), dtype=int)\n",
    "    y[class_dict[specialty]] = 1\n",
    "    return y\n",
    "\n",
    "encoded_specialties = df.medical_specialty.apply(one_hot_encode).tolist()\n",
    "y = np.stack(encoded_specialties)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Create vocabulary with special tokens\n",
    "special_tokens = [\"<PAD>\", \"<UNK>\"]\n",
    "word_to_idx = {word: idx+len(special_tokens) for idx, word in enumerate(vocab_filtered)}\n",
    "for idx, token in enumerate(special_tokens):\n",
    "    word_to_idx[token] = idx\n",
    "vocab_size = len(word_to_idx)\n",
    "\n",
    "# Convert texts to sequences of indices\n",
    "def tokenize(text):\n",
    "    return [word_to_idx.get(word, word_to_idx[\"<UNK>\"]) for word in text.split()]\n",
    "\n",
    "# Create a custom dataset\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = [tokenize(text) for text in texts]\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.texts[idx]), self.labels[idx]\n",
    "\n",
    "# Collate function for padding sequences\n",
    "def collate_batch(batch):\n",
    "    texts, labels = zip(*batch)\n",
    "    texts_padded = pad_sequence([torch.tensor(x) for x in texts], batch_first=True, padding_value=word_to_idx[\"<PAD>\"])\n",
    "    return texts_padded, torch.tensor(labels)\n",
    "\n",
    "# Random split\n",
    "indices = list(range(len(df)))\n",
    "random.shuffle(indices)  # Shuffle the indices\n",
    "\n",
    "# Calculate split point (85% train, 15% test)\n",
    "split_idx = int(0.85 * len(indices))\n",
    "train_indices = indices[:split_idx]\n",
    "test_indices = indices[split_idx:]\n",
    "\n",
    "y_train = y[train_indices]\n",
    "y_test = y[test_indices]\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = TextDataset(df.iloc[train_indices]['transcription_clean'], y_train.argmax(axis=1))\n",
    "test_dataset = TextDataset(df.iloc[test_indices]['transcription_clean'], y_test.argmax(axis=1))\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class EmbeddingClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, num_layers=1, bidirectional=False, dropout=0.5):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # RNN layer (using LSTM)\n",
    "        self.rnn = nn.LSTM(embedding_dim, \n",
    "                           hidden_dim, \n",
    "                           num_layers=num_layers,\n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout=dropout if num_layers > 1 else 0,\n",
    "                           batch_first=True)\n",
    "        \n",
    "        # Output layer\n",
    "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        # text shape: [batch_size, seq_len]\n",
    "        \n",
    "        # Apply embedding layer\n",
    "        embedded = self.embedding(text)  # [batch_size, seq_len, embedding_dim]\n",
    "        \n",
    "        # Pass through RNN\n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        \n",
    "        # If bidirectional, concatenate the final forward and backward hidden states\n",
    "        if self.rnn.bidirectional:\n",
    "            hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "        else:\n",
    "            hidden = hidden[-1,:,:]\n",
    "            \n",
    "        # Apply dropout\n",
    "        hidden = self.dropout(hidden)\n",
    "        \n",
    "        # Pass through linear layer\n",
    "        return self.fc(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "embedding_dim = 32\n",
    "hidden_dim = 32\n",
    "model = EmbeddingClassifier(vocab_size, embedding_dim, hidden_dim, len(classes), bidirectional=False)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for texts, labels in train_dataloader:\n",
    "        # Forward pass\n",
    "        outputs = model(texts)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_dataloader):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, correct = 0, 0\n",
    "with torch.no_grad():\n",
    "    for texts, labels in test_dataloader:\n",
    "        outputs = model(texts)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        preds = torch.argmax(outputs, axis=1)\n",
    "        correct += (preds == labels).sum()\n",
    "test_loss = test_loss/len(test_dataloader)\n",
    "accuracy = correct / len(y_test)\n",
    "print(f\"Test loss: {test_loss:.4f}\")\n",
    "print(f\"Test accuracy: {accuracy:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
