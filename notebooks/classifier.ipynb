{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "646b04e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddbcdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/mtsamples.csv\")\n",
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86274365",
   "metadata": {},
   "source": [
    "Because of the long tail, going to focus on top specialties only, say with >100 samples. Note there is a kind of inconsistent semantics here with visit types mixed in, we'll take a look at that in error analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ee3c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_specialties = df.medical_specialty.value_counts()\n",
    "top_specialties = top_specialties[top_specialties > 100]\n",
    "top_specialties\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7006c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = list(top_specialties.keys())\n",
    "prev_count = len(df)\n",
    "df = df.loc[df.medical_specialty.isin(classes)]\n",
    "print(f\"Went from {prev_count} samples to {len(df)} samples\")\n",
    "\n",
    "class_dict = {c:i for i, c in enumerate(classes)}\n",
    "\n",
    "def one_hot_encode(specialty):\n",
    "    y = np.zeros(len(classes), dtype=int)\n",
    "    y[class_dict[specialty]] = 1\n",
    "    return y\n",
    "\n",
    "df[\"target\"] = df.medical_specialty.apply(one_hot_encode)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cb97a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-val-test\n",
    "# df[\"split\"] = random.choices(\n",
    "#     [\"train\", \"val\", \"test\"],\n",
    "#     weights=[0.7, 0.15, 0.15],\n",
    "#     k=len(df)\n",
    "# )\n",
    "\n",
    "# Create indices for train-test split\n",
    "indices = np.arange(len(df))\n",
    "np.random.shuffle(indices)  # Shuffle the indices\n",
    "\n",
    "# Calculate split point (85% train, 15% test)\n",
    "split_idx = int(0.85 * len(indices))\n",
    "train_indices = indices[:split_idx]\n",
    "test_indices = indices[split_idx:]\n",
    "\n",
    "len(train_indices), len(test_indices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5646c43a",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "- lower case\n",
    "- remove punctuation and symbols\n",
    "- remove stopwords\n",
    "\n",
    "# Featurization\n",
    "- BoW\n",
    "- TF-IDF\n",
    "- Embedding layer\n",
    "- Pre-trained embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b131b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Make sure stopwords are downloaded\n",
    "    try:\n",
    "        nltk.data.find('corpora/stopwords')\n",
    "    except LookupError:\n",
    "        nltk.download('stopwords')\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    \n",
    "    return text\n",
    "\n",
    "df['processed_transcription'] = df['transcription'].apply(preprocess_text)\n",
    "\n",
    "# Display a sample to verify the preprocessing\n",
    "print(\"Original vs Processed example:\")\n",
    "sample_idx = 0\n",
    "print(\"Original:\", df['transcription'].iloc[sample_idx][:100], \"...\")\n",
    "print(\"Processed:\", df['processed_transcription'].iloc[sample_idx][:100], \"...\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290fed9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {}\n",
    "for _, row in df.iterrows():\n",
    "    x = row[\"processed_transcription\"]\n",
    "    for word in x.split():\n",
    "        vocab[word] = vocab.get(word, 0) + 1\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b56fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert vocab to a pandas Series for easier manipulation\n",
    "word_counts = pd.Series(vocab)\n",
    "\n",
    "# Sort by frequency (descending)\n",
    "word_counts = word_counts.sort_values(ascending=False)\n",
    "\n",
    "# Plot the distribution of the top 50 words\n",
    "plt.figure(figsize=(12, 6))\n",
    "word_counts.head(50).plot(kind='bar')\n",
    "plt.title('Top 50 Words by Frequency')\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot the overall distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(len(word_counts)), word_counts.values)\n",
    "plt.title('Word Frequency Distribution')\n",
    "plt.xlabel('Word Rank')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display the top 20 words\n",
    "word_counts.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fb6b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10  \n",
    "vocab_filtered = [word for word in vocab if vocab[word] > k]  # same order of magnitude as n\n",
    "p = len(vocab_filtered)\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c648ffd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(df)\n",
    "\n",
    "X = np.zeros((n,p))\n",
    "y = np.zeros((n,len(classes)))\n",
    "\n",
    "from collections import Counter\n",
    "def get_word_counts(s):\n",
    "    freqs = Counter()\n",
    "    for word in s.split():\n",
    "        freqs[word] += 1\n",
    "    return freqs\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    doc = row.processed_transcription\n",
    "    freqs = get_word_counts(doc)\n",
    "    x_bow = np.zeros(p)\n",
    "    for j in range(p):\n",
    "        word = vocab_filtered[j]\n",
    "        x_bow[j] = freqs[word]\n",
    "    X[i:] = x_bow\n",
    "    y[i:] = row.target\n",
    "X, y\n",
    "\n",
    "# def get_bow_vector_binary(document):\n",
    "#     return [1 if word in document else 0 for word in vocab_filtered]\n",
    "\n",
    "# df[\"bow_binary\"] = df.processed_transcription.apply(get_bow_vector_binary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cf2219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate sparsity of X\n",
    "# Sparsity is the percentage of zero elements in the matrix\n",
    "sparsity = np.count_nonzero(X == 0) / X.size\n",
    "print(f\"Sparsity of X: {sparsity:.4f} ({sparsity*100:.2f}%)\")\n",
    "\n",
    "# Alternatively, we can calculate density (non-zero elements)\n",
    "density = 1 - sparsity\n",
    "print(f\"Density of X: {density:.4f} ({density*100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde2b58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X[train_indices], y[train_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "f0c51f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Linear\n",
    "\n",
    "class LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.linear = Linear(input_dim, output_dim)\n",
    "        # self.softmax = torch.nn.Softmax(dim=1)  # don't need to use softmax in forward if using crossentropyloss\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.hidden = Linear(input_dim, hidden_dim)\n",
    "        self.linear = Linear(hidden_dim, output_dim)\n",
    "        # self.softmax = torch.nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.hidden(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        return x self.linear(x)\n",
    "        # return self.softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "0797a1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = X[train_indices], X[test_indices]\n",
    "y_train, y_test = y[train_indices], y[test_indices]\n",
    "\n",
    "X_train_tensor = torch.FloatTensor(X_train)\n",
    "y_train_tensor = torch.LongTensor(y_train.argmax(axis=1))\n",
    "X_test_tensor = torch.FloatTensor(X_test)\n",
    "y_test_tensor = torch.LongTensor(y_test.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7c002a",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, true_labels = torch.max(y_test_tensor, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "8dda53ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model_type, hidden_dim_factor, learning_rate, weight_decay, batch_size, iterations):\n",
    "    p = X_train.shape[1]\n",
    "\n",
    "    if model_type == \"logistic\":\n",
    "        model = LogisticRegression(p, len(classes))\n",
    "    else:\n",
    "        hidden_dim = int(p * hidden_dim_factor)\n",
    "        model = MLP(p, hidden_dim, len(classes))\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    X_train_tensor = torch.FloatTensor(X[train_indices])\n",
    "    y_train_tensor = torch.LongTensor(y[train_indices].argmax(axis=1))\n",
    "    train_losses, val_losses = [], []\n",
    "\n",
    "    for _ in range(iterations):\n",
    "        # Sample a random batch\n",
    "        indices = torch.randperm(len(X_train_tensor))[:batch_size]\n",
    "        X_batch = X_train_tensor[indices]\n",
    "        y_batch = y_train_tensor[indices]\n",
    "        \n",
    "        # Forward model on batch\n",
    "        preds = model(X_batch)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = loss_function(preds, y_batch)\n",
    "\n",
    "        # Backpropagate\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Take step\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "        \n",
    "        # Create a validation set (using a different random subset)\n",
    "        val_indices = torch.randperm(len(X_train_tensor))[:batch_size*2]\n",
    "        X_val = X_train_tensor[val_indices]\n",
    "        y_val = y_train_tensor[val_indices]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_preds = model(X_val)\n",
    "            val_loss = loss_function(val_preds, y_val).item()\n",
    "            val_losses.append(val_loss)\n",
    "\n",
    "    # Evaluate on test set\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_test_tensor)\n",
    "        \n",
    "    _, predicted = torch.max(y_pred, dim=1)\n",
    "    # _, true_labels = torch.max(y_test_tensor, dim=1)\n",
    "    accuracy = (predicted == y_test_tensor).float().mean().item()\n",
    "\n",
    "    return {\n",
    "        'model_type': model_type,\n",
    "        'hidden_dim_factor': hidden_dim_factor,\n",
    "        'learning_rate': learning_rate,\n",
    "        'weight_decay': weight_decay,\n",
    "        'batch_size': batch_size,\n",
    "        'iterations': iterations,\n",
    "        'final_train_loss': train_losses[-1],\n",
    "        'final_val_loss': val_losses[-1] if val_losses else None,\n",
    "        'test_accuracy': accuracy\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "82799a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter grid\n",
    "param_grid = {\n",
    "    'model_type': ['logistic', 'mlp'],\n",
    "    'hidden_dim_factor': [0.1, 0.5, 1.0, 2.0],  # as a fraction of input_dim\n",
    "    'learning_rate': [0.001, 0.01, 0.1],\n",
    "    'weight_decay': [0.0, 0.001, 0.01, 0.1],\n",
    "    'batch_size': [64, 128, 256],\n",
    "    'iterations': [500, 1000]  # Reduced for faster execution\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "def run_hyperparameter_sweep(param_grid, num_runs=5):\n",
    "    from itertools import product\n",
    "    # Generate parameter combinations to test\n",
    "    # For a full grid search, uncomment the following:\n",
    "    # param_combinations = list(product(*param_grid.values()))\n",
    "    \n",
    "    # For a random subset to save time:\n",
    "    all_combinations = list(product(*param_grid.values()))\n",
    "    np.random.shuffle(all_combinations)\n",
    "    param_combinations = all_combinations[:num_runs]\n",
    "    \n",
    "    for i, params in enumerate(param_combinations):\n",
    "        print(f\"Run {i+1}/{len(param_combinations)}: {dict(zip(param_grid.keys(), params))}\")\n",
    "        \n",
    "        # Unpack parameters\n",
    "        model_type, hidden_dim_factor, learning_rate, weight_decay, batch_size, iterations = params\n",
    "        \n",
    "        # Train and evaluate model\n",
    "        result = train_and_evaluate(\n",
    "            model_type, hidden_dim_factor, learning_rate, weight_decay, batch_size, iterations\n",
    "        )\n",
    "        \n",
    "        results.append(result)\n",
    "    \n",
    "    # Convert results to DataFrame for easier analysis\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1169642",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = run_hyperparameter_sweep(param_grid, num_runs=10)\n",
    "# Display top 5 models by test accuracy\n",
    "results_df.sort_values('test_accuracy', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7abbea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[np.argmax([r[\"test_accuracy\"] for r in results])]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
